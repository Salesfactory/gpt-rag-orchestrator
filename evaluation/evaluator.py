"""
Evaluator module for grading RAG responses using GPT-4.1 with structured outputs.
"""

import os
from pathlib import Path
from typing import Optional
from pydantic import BaseModel, Field
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)


class EvaluationResult(BaseModel):
    """Structured evaluation result from GPT-4.1."""

    score: int = Field(
        description="Score from 1-5 where: "
        "5 = Perfectly accurate and complete answer, "
        "4 = Correct but missing minor details, "
        "3 = Partially correct with some inaccuracies, "
        "2 = Major inaccuracies or incomplete, "
        "1 = Completely wrong or hallucinated",
        ge=1,
        le=5
    )
    reasoning: str = Field(
        description="Detailed explanation of the score and what was correct or incorrect"
    )
    factual_accuracy: str = Field(
        description="Overall accuracy assessment",
        pattern="^(accurate|partially accurate|inaccurate)$"
    )


class RAGEvaluator:
    """Evaluates RAG responses against ground truth using GPT-4.1."""

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the evaluator.

        Args:
            api_key: OpenAI API key (defaults to OPENAI_API_KEY from .env)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError(
                "API key is required. "
                "Set OPENAI_API_KEY in .env file or pass api_key parameter."
            )

        self.client = OpenAI(api_key=self.api_key)

    def evaluate(self, question: str, fact: str, ai_answer: str) -> EvaluationResult:
        """
        Evaluate an AI answer against the ground truth fact.

        Args:
            question: The original question asked
            fact: The ground truth answer (expected answer)
            ai_answer: The answer generated by the RAG system

        Returns:
            EvaluationResult with score, reasoning, and accuracy assessment
        """
        system_prompt = """You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems.

Your task is to compare an AI-generated answer against a ground truth fact and grade its accuracy.

Grading rubric:
- Score 5: The answer is perfectly accurate, complete, and fully matches the fact
- Score 4: The answer is correct but may be missing minor details or have slightly different phrasing
- Score 3: The answer is partially correct but contains some inaccuracies or omissions
- Score 2: The answer has major inaccuracies or is significantly incomplete
- Score 1: The answer is completely wrong, contradicts the fact, or is a hallucination

Be objective and fair in your evaluation. Consider:
1. Factual correctness
2. Completeness
3. Relevance to the question
4. Whether the answer addresses what was asked"""

        user_prompt = f"""Question: {question}

Ground Truth (Fact): {fact}

AI Answer: {ai_answer}

Evaluate the AI answer against the ground truth fact and provide your assessment."""

        try:
            response = self.client.responses.parse(
                model="gpt-4.1",
                input=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                text_format=EvaluationResult,
            )

            return response.output_parsed

        except Exception as e:
            return EvaluationResult(
                score=0,
                reasoning=f"Evaluation failed: {str(e)}",
                factual_accuracy="inaccurate"
            )
